# The list of papers with small description of their role in our approach:

1. [An Empirical Study of Mamba-based Language Models](https://arxiv.org/abs/2406.07887) - Wide experimental base with hybrid mamba and transformer in LLMs.
1. [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) - Mamba-1.
1. [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060) - Mamba-2.
1. [Activating More Pixels in Image Super-Resolution Transformer](https://arxiv.org/abs/2205.04437) - HAT first paper. Our approach is an evalution of HAT.
1. [HAT: Hybrid Attention Transformer for Image Restoration](https://arxiv.org/abs/2309.05239) - HAT, revisited paper.
1. [NTIRE 2024 Challenge on Image Super-Resolution (Ã—4): Methods and Results](https://arxiv.org/abs/2404.09790) - The first mention of the Contrast confirms that Contrast was one of the earliest hybrid models of Transformers and Mamba, and our paper is a refined idea of this initial attempt.
2. [Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing](https://arxiv.org/abs/2306.12929) - Clipped Softmax for stabilizing transformers.
3. [MambaIR: A Simple Baseline for Image Restoration with State-Space Model](https://arxiv.org/abs/2402.15648) - Mention of diagonal information forgetting in Mamba.
4. [VMamba: Visual State Space Model](https://arxiv.org/abs/2401.10166) - VMamba, we use their mamba blocks as a baseline
5. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Creators of transformers
6. [Mamba-R: Vision Mamba ALSO Needs Registers](https://arxiv.org/abs/2405.14858) - Mamba with Registers
7. [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588) - Vision Transformers with Registers
