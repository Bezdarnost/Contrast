Dear Aman,

Sorry for the late reply. I have carefully read your email sent on May 16th.

I believe it would be more suitable for you to lead the collaboration for the following reasons:

    I have recently joined Professor Yulun Zhang's lab, and my main focus needs to be on the lab's projects, leaving me with limited time and energy for another new collaboration.

    The new projects in the lab might not align well with your research and, therefore, might not fully utilize your strengths.

    Professor Zhang prefers not to involve non-lab members in lab projects.

In a collaboration led by you, I can offer the following support:

    Discussions on ideas, techniques, experimental analysis, etc.

    I can provide 2-4 A100 (or A800) GPUs, depending on availability. However, I cannot give you an account for direct access due to lab requirements. Instead, you can submit your code to GitHub, and I will run the experiments and provide you with the results. (P.S.: Considering the potential delay in this process, I recommend using it for larger experiments rather than testing.)

My thoughts on your ideas:

    "Contrast" model (Convolutional Transformer State Space Model) â€“ I'm not entirely sure I understand what this word means. Does it refer to CNN+Transformer?

    For idea-1: Mamba+Transformer has great potential. (P.S.: I am not familiar with Mamba yet, so there might be issues with my further ideas.)

    For idea-2: Improvements on softmax are worth exploring, but from an innovation (paper) perspective, I don't think this should be the main focus.

    For idea-3: Rewriting convolution might lack innovation if it's just an engineering implementation. However, it could be worthwhile if it's related to improvements in Mamba.

    For idea-4: Additional weight sounds like bias, which is already extensively explored in Transformers.

    For idea-5: Although this idea hasn't been explicitly presented in previous papers, many methods use Conv in MLPs. Additionally, Linear can be seen as 1x1conv, so further exploration in this direction might be limited.

    For idea-6: I'm not an expert in experimental aspects. If you are proficient with ThunderKittens, you can use it.

Other thoughts:

    From your ideas, I see two main areas: Mamba and convolution. Mamba, being an emerging method, is worth exploring. The convolution designs mostly seem engineering-oriented, which might improve performance and speed but offer limited innovation.

    I suggest focusing on Mamba. If the convolution improvements can be utilized in Mamba exploration, they are worth trying. However, as standalone improvements, they might not be suitable.

    These are just my perspectives. I may not fully understand your ideas. They are for reference only.

Collaboration format (preliminary ideas):

    Private, shared GitHub repository.

    Google Docs to record our tasks, papers, ideas, experiments, etc.

P.S. I apologize for the delayed response. I have an exam to prepare for (on May 30th, Beijing time) and other tasks from Professor Zhang.


Best regards, Zheng Chen
